{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R^2 Score: 0.6116277684913793\n",
      "Correlation Coefficient: 0.7820663453258805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/svm/_base.py:246: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn('Solver terminated early (max_iter=%i).'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def simulate_nd_space_and_regression(n_dimensions, p_points):\n",
    "    # Simulate an N-dimensional space with p points, each dimension normally distributed\n",
    "    X = np.random.normal(size=(p_points, n_dimensions))\n",
    "    \n",
    "    # Simulate a response variable Y, which is also normally distributed and independent from X\n",
    "    Y = np.random.normal(size=p_points)\n",
    "\n",
    "    # Perform linear regression\n",
    "    #model = LinearRegression()\n",
    "    model = SVR(kernel='linear',max_iter=1000,\n",
    "                        C=0.01)\n",
    "    model.fit(X, Y)\n",
    "\n",
    "    # Predict and compute the R^2 score (correlation coefficient squared)\n",
    "    Y_pred = model.predict(X)\n",
    "    r2 = r2_score(Y, Y_pred)\n",
    "    \n",
    "    # Compute the correlation coefficient (sqrt of R^2)\n",
    "    correlation_coefficient = np.sqrt(r2)\n",
    "    \n",
    "    return r2, correlation_coefficient\n",
    "\n",
    "# Example usage\n",
    "n_dimensions = 256  # Number of dimensions\n",
    "p_points = 300    # Number of points\n",
    "\n",
    "r2, correlation_coefficient = simulate_nd_space_and_regression(n_dimensions, p_points)\n",
    "\n",
    "print(f\"R^2 Score: {r2}\")\n",
    "print(f\"Correlation Coefficient: {correlation_coefficient}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NB : les dimensions ne sont pas complètement indépendantes\n",
    "## -> Use the real distribution and shuffle the labels.\n",
    "## -> Is it better with V0 than V1 ?\n",
    "## But there is a train / test and a cross validation ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-22.691949519898582\n",
      "-17.56000277866854\n",
      "-4.69426493175534\n",
      "-8.425072063814703\n",
      "-8.21757808731576\n",
      "-5.158715697133647\n",
      "-11.166528390839716\n",
      "-6.078174740630961\n",
      "-5.979388901874851\n",
      "-16.37862284398496\n",
      "Mean R^2 Score from 10-fold CV: -10.635029795591706\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def simulate_nd_space_and_regression(n_dimensions, p_points):\n",
    "    # Simulate an N-dimensional space with p points, each dimension normally distributed\n",
    "    X = np.random.normal(size=(p_points, n_dimensions))\n",
    "    \n",
    "    # Simulate a response variable Y, which is also normally distributed and independent from X\n",
    "    Y = np.random.normal(size=p_points)\n",
    "\n",
    "    # Perform regression using Support Vector Regression (SVR)\n",
    "    model = SVR(kernel='linear')\n",
    "\n",
    "    # Perform 10-fold cross-validation\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    r2_scores = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "        model.fit(X_train, Y_train)\n",
    "        Y_pred = model.predict(X_test)\n",
    "        r2 = r2_score(Y_test, Y_pred)\n",
    "        r2_scores.append(r2)\n",
    "        print(r2)\n",
    "\n",
    "    # Compute the mean R^2 score\n",
    "    mean_r2 = np.mean(r2_scores)\n",
    "\n",
    "    return mean_r2\n",
    "\n",
    "# Example usage\n",
    "n_dimensions = 256  # Number of dimensions\n",
    "p_points = 300    # Number of points\n",
    "\n",
    "mean_r2 = simulate_nd_space_and_regression(n_dimensions, p_points)\n",
    "\n",
    "print(f\"Mean R^2 Score from 10-fold CV: {mean_r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## perform CCA !\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embs = pd.read_csv('/neurospin/dico/jlaval/Output/SC-sylv_12-16/16-38-32_170/troiani_custom_embeddings/custom_cross_val_embeddings.csv')\n",
    "#embs = pd.read_csv('/neurospin/dico/jlaval/Output/SC-sylv_12-16/16-39-27_30/troiani_custom_embeddings/custom_cross_val_embeddings.csv')\n",
    "#embs = pd.read_csv('/neurospin/dico/jlaval/Output/SC-sylv_left_v1/keep_bottom/troiani_custom_embeddings/custom_cross_val_embeddings.csv') ## V1 before even changing cutin prop\n",
    "embs = pd.read_csv('/neurospin/dico/jlaval/Output/SC-sylv_left_v1/no_keep_bottom/troiani_custom_embeddings/custom_cross_val_embeddings.csv') ## same here = best ?\n",
    "#embs = pd.read_csv('/volatile/jl277509/Runs/02_STS_babies/Program/Output/SC-sylv_isomaps/16-40-54_148/troiani_embeddings/custom_cross_val_embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "isomap = pd.read_csv('/neurospin/dico/data/deep_folding/current/datasets/hcp/hcp_isomap_labels.csv')\n",
    "embs.columns=['Subject']+embs.columns[1:].tolist()\n",
    "merged = pd.merge(isomap, embs)\n",
    "cols_embs = [f'dim{k}' for k in range(1,33)]\n",
    "cols_iso = [f'Isomap_central_left_dim{k}' for k in range(1,7)]\n",
    "embs, isomap = merged[cols_embs].to_numpy(), merged[cols_iso].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3810063460197253 2.8785028180120236\n",
      "-0.5561476711573823 2.568573698964332\n",
      "-0.6039396072569791 2.2065305357893705\n",
      "-0.36907427158893086 2.024278734702382\n",
      "0.18555799005356918 1.6367642162976548\n",
      "0.09503153515159224 1.5360945034042632\n"
     ]
    }
   ],
   "source": [
    "# standardized ?\n",
    "for k in range(6):\n",
    "    print(np.mean(isomap[:,k]), np.std(isomap[:,k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3.469446951953614e-18, 0.9999999999997334)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=32)\n",
    "arr = pca.fit_transform(embs)\n",
    "pearsonr(arr[:, 0],arr[:, 26])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB : Isomap components are not standardized -> should they be ?\n",
    "does the variance reflect the importance of the component ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.0651325446315982, 0.22286584578684657)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# are they uncorrelated ?\n",
    "# and : does orthogonal directions imply independance of the obtained variables ?\n",
    "from scipy.stats import pearsonr\n",
    "pearsonr(isomap[:,0], isomap[:,1])\n",
    "# they seem uncorelated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5576301757276653\n"
     ]
    }
   ],
   "source": [
    "cca = CCA(n_components=len(cols_iso), scale=False) ## need to set scale to false !!\n",
    "cca.fit_transform(embs, isomap)\n",
    "print(cca.score(embs, isomap))\n",
    "## NB: faut-il faire un train val test pour avoir le vrai score ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(352, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isomap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06584258093794765 0.005963065767634256 0.07521839539895217\n"
     ]
    }
   ],
   "source": [
    "# compare to a random shuffle of the labels\n",
    "scores = []\n",
    "for k in range(100):\n",
    "    np.random.shuffle(isomap)\n",
    "    cca.fit_transform(embs, isomap)\n",
    "    scores.append(cca.score(embs, isomap))\n",
    "print(np.mean(scores), np.std(scores), np.quantile(scores, 0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02489570350657903\n",
      "(0.4289645935117821, 3.4407495237434334e-17)\n"
     ]
    }
   ],
   "source": [
    "## Try only one component = find best correlation between one direction in latent space and one direction in isomap space\n",
    "cca = CCA(n_components=1, scale=False)\n",
    "cca.fit(embs, isomap)\n",
    "print(cca.score(embs, isomap))\n",
    "\n",
    "embs_c, isomap_c = cca.transform(embs, isomap)\n",
    "print(pearsonr(embs_c.reshape(-1), isomap_c.reshape(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22627644121106594"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(embs_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02745890704373823\n",
      "0.00974655562484734 0.0008706855078084492 0.011143903083616112\n",
      "0.20040945337058655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/cross_decomposition/_pls.py:96: ConvergenceWarning: Maximum number of iterations reached\n",
      "  warnings.warn('Maximum number of iterations reached',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028225937581078372 0.0024735042060606473 0.032371931838130426\n",
      "0.1643822995104394\n",
      "0.02826037617676207 0.002653058806466211 0.03351423360543796\n",
      "0.18972458796250047\n",
      "0.028944428950306113 0.0028124928247359984 0.034052614511771356\n",
      "0.18483657268917217\n",
      "0.03045062065944876 0.0029466681966070314 0.03587173810740223\n",
      "0.21535903953819746\n",
      "0.032956817245809254 0.003251310003655106 0.03786984345905486\n"
     ]
    }
   ],
   "source": [
    "len(cols_iso)data/deep_folding/current/datasets/hcp/hcp_isomap_labels.csv')\n",
    "    embs.columns=['Subject']+embs.columns[1:].tolist()\n",
    "    merged = pd.merge(isomap, embs)\n",
    "    cols_embs = [f'dim{k}' for k in range(1,33)]\n",
    "    cols_iso = [f'Isomap_central_left_dim{k}' for k in range(1,7)]\n",
    "    embs, isomap = merged[cols_embs].to_numpy(), merged[cols_iso].to_numpy()\n",
    "    cca = CCA(n_components=len(cols_iso))\n",
    "    cca.fit_transform(embs, isomap)\n",
    "    print(cca.score(embs, isomap))\n",
    "\n",
    "    # compare to a random shuffle of the labels\n",
    "    scores = []\n",
    "    for k in range(100):\n",
    "        np.random.shuffle(isomap)\n",
    "        cca.fit_transform(embs, isomap)\n",
    "        scores.append(cca.score(embs, isomap))\n",
    "    print(np.mean(scores), np.std(scores), np.quantile(scores, 0.95))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "## What happens if perfect correlation ?\n",
    "#embs[:, :6] = isomap\n",
    "# get a perfect score of 1. (checked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CCA between latent spaces ?\n",
    "embs1 = pd.read_csv('/neurospin/dico/jlaval/Output/ABLATION_FIP/no_cutout/FIP_right_custom_embeddings/custom_cross_val_embeddings.csv').to_numpy()\n",
    "embs2 = pd.read_csv('/neurospin/dico/jlaval/Output/ABLATION_FIP/no_cutout_4/FIP_right_custom_embeddings/custom_cross_val_embeddings.csv').to_numpy()\n",
    "\n",
    "embs1 = pd.read_csv('/neurospin/dico/jlaval/Output/ABLATION_FIP/all_augms_0/FIP_right_custom_embeddings/custom_cross_val_embeddings.csv').to_numpy()\n",
    "embs2 = pd.read_csv('/neurospin/dico/jlaval/Output/ABLATION_FIP/all_augms_4/FIP_right_custom_embeddings/custom_cross_val_embeddings.csv').to_numpy()\n",
    "\n",
    "embs1 = pd.read_csv('/neurospin/dico/jlaval/Output/FIP_right_regular_augm/cutin_80_2/FIP_right_custom_embeddings/custom_cross_val_embeddings.csv').to_numpy()\n",
    "embs2 = pd.read_csv('/neurospin/dico/jlaval/Output/FIP_right_regular_augm/cutin_80_3/FIP_right_custom_embeddings/custom_cross_val_embeddings.csv').to_numpy()\n",
    "\n",
    "#embs1 = pd.read_csv('/neurospin/dico/jlaval/Output/ABLATION_FIP/no_cutout/42433_ukb_FIP_right_random_embeddings/full_embeddings.csv').to_numpy()[:,1:]\n",
    "#embs2 = pd.read_csv('/neurospin/dico/jlaval/Output/ABLATION_FIP/no_cutout_2/42433_ukb_FIP_right_random_embeddings/full_embeddings.csv').to_numpy()[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/cross_decomposition/_pls.py:96: ConvergenceWarning: Maximum number of iterations reached\n",
      "  warnings.warn('Maximum number of iterations reached',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8923692710910487\n",
      "-0.7826809260576133\n"
     ]
    }
   ],
   "source": [
    "cca = CCA(n_components=32)\n",
    "cca.fit(embs1, embs2)\n",
    "print(cca.score(embs1, embs2))\n",
    "\n",
    "scores=[]\n",
    "for k in range(1):\n",
    "    np.random.shuffle(embs1)\n",
    "    cca.fit(embs1, embs2)\n",
    "    scores.append(cca.score(embs1, embs2))\n",
    "print(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A calculer sur UKB plutôt !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CCA between latent space and morphometric measurements\n",
    "# NB : in high dimension, is imposing full orthogonality between variables the best way to proceed ? Will we underestimate the correlation ?\n",
    "# CCA seems better than linear CKA for this specific study ! CKA is better for comparing latent spaces. What about isomap ? Unclear yet.\n",
    "# indeed, for morpho, the morpho params may not be of high importance in the latent space, so the rescaling seems important to catch them ?\n",
    "\n",
    "# seems like the values are very low anyway, so it is not interesting to compare the spaces linearly ...\n",
    "# no significant decorrelation with depth using trimdepth ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "morpho = pd.read_csv('/neurospin/dico/data/deep_folding/current/datasets/UkBioBank/sulcal_morphometry/UKB_21045_FIP_right.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = pd.read_csv('/neurospin/dico/jlaval/Output/ABLATION_FIP/no_trimdepth_4/42433_ukb_FIP_right_random_embeddings/full_embeddings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = embeddings.loc[embeddings['ID'].isin(morpho['Subject'])]\n",
    "morpho = morpho.loc[morpho['Subject'].isin(embeddings['ID'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = embeddings.to_numpy()[: ,1:]\n",
    "y = morpho.to_numpy()[:, 4:].astype('float')\n",
    "#y = morpho[['surface_talairach', 'meandepth_talairach', 'hull_junction_length_talairach']].to_numpy()\n",
    "#y = morpho[['meandepth_talairach']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.25296534738801907\n"
     ]
    }
   ],
   "source": [
    "cca = CCA(n_components=y.shape[1], scale=True)\n",
    "cca.fit(X, y)\n",
    "print(cca.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surface_talairach\n",
      "0.024227285168421542\n",
      "surface_native\n",
      "0.02662625490004411\n",
      "maxdepth_talairach\n",
      "0.04313500532838188\n",
      "maxdepth_native\n",
      "0.03441156259702827\n",
      "meandepth_talairach\n",
      "0.1550896720193533\n",
      "meandepth_native\n",
      "0.13559090919978345\n",
      "hull_junction_length_talairach\n",
      "0.05405337534768939\n",
      "hull_junction_length_native\n",
      "0.06274162118002058\n",
      "GM_thickness\n",
      "0.012143048865606909\n",
      "opening\n",
      "0.011950139905762769\n"
     ]
    }
   ],
   "source": [
    "# each var separately\n",
    "for var in morpho.columns[4:]:\n",
    "    print(var)\n",
    "    y = morpho[[var]].to_numpy()\n",
    "    cca = CCA(n_components=y.shape[1], scale=True)\n",
    "    cca.fit(X, y)\n",
    "    print(cca.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21001, 3)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now restrain to some variables\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
