{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import omegaconf\n",
    "import glob\n",
    "import hydra\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.ndimage import zoom\n",
    "from omegaconf import DictConfig\n",
    "from omegaconf import OmegaConf\n",
    "from contrastive.utils.config import process_config\n",
    "from contrastive.data.datamodule import DataModule_Evaluation\n",
    "from contrastive.models.contrastive_learner_fusion import \\\n",
    "    ContrastiveLearnerFusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/volatile/jl277509/Runs/02_STS_babies/Program/Output/ORBITAL_12-layer_k7/16-19-26_238'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:contrastive.utils.config: Working directory : /volatile/jl277509/Runs/02_STS_babies/Program/2023_jlaval_STSbabies/contrastive/notebooks/julien/visualization\n",
      "INFO:utils.py: Train/val size partitions = [18946, 2105]\n",
      "INFO:utils.py: Seed for train/val split is 1\n",
      "INFO:utils.py: test set size: (0, 30, 38, 22, 1)\n",
      "INFO:utils.py: Length of train dataframe = 18946\n",
      "INFO:utils.py: Length of val dataframe = 2105\n",
      "INFO:utils.py: Length of train_val dataframe = 21051\n",
      "INFO:create_datasets.py: foldlabel data NOT requested. Foldlabel data NOT loaded\n",
      "INFO:create_datasets.py: distbottom data NOT requested. Distbottom data NOT loaded\n",
      "INFO:contrastive_learner_fusion.py: n_datasets 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No trained_model.pt saved. Create a new instance and load weights.\n"
     ]
    }
   ],
   "source": [
    "config_path = os.path.join(model_path, '.hydra/config.yaml')\n",
    "config = omegaconf.OmegaConf.load(config_path)\n",
    "config = process_config(config)\n",
    "\n",
    "config.apply_augmentations = False\n",
    "config.with_labels = False\n",
    "\n",
    "# create new models in mode visualisation\n",
    "data_module = DataModule_Evaluation(config)\n",
    "data_module.setup(stage='validate')\n",
    "\n",
    "# create a new instance of the current model version,\n",
    "# then load hydra weights.\n",
    "print(\"No trained_model.pt saved. Create a new instance and load weights.\")\n",
    "\n",
    "model = ContrastiveLearnerFusion(config, sample_data=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContrastiveLearnerFusion(\n",
       "  (backbones): ModuleList(\n",
       "    (0): ConvNet(\n",
       "      (encoder): Sequential(\n",
       "        (conv0): Conv3d(1, 32, kernel_size=(7, 7, 7), stride=(1, 1, 1), padding=(3, 3, 3))\n",
       "        (norm0): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyReLU0): LeakyReLU(negative_slope=0.01)\n",
       "        (DropOut0): Dropout3d(p=0.05, inplace=False)\n",
       "        (conv0a): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (norm0a): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyReLU0a): LeakyReLU(negative_slope=0.01)\n",
       "        (DropOut0a): Dropout3d(p=0.05, inplace=False)\n",
       "        (conv0b): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (norm0b): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyReLU0b): LeakyReLU(negative_slope=0.01)\n",
       "        (DropOut0b): Dropout3d(p=0.05, inplace=False)\n",
       "        (conv0c): Conv3d(32, 32, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "        (norm0c): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyReLU0c): LeakyReLU(negative_slope=0.01)\n",
       "        (DropOut0c): Dropout3d(p=0.05, inplace=False)\n",
       "        (conv1): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (norm1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyReLU1): LeakyReLU(negative_slope=0.01)\n",
       "        (DropOut1): Dropout3d(p=0.05, inplace=False)\n",
       "        (conv1a): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (norm1a): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyReLU1a): LeakyReLU(negative_slope=0.01)\n",
       "        (DropOut1a): Dropout3d(p=0.05, inplace=False)\n",
       "        (conv1b): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (norm1b): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyReLU1b): LeakyReLU(negative_slope=0.01)\n",
       "        (DropOut1b): Dropout3d(p=0.05, inplace=False)\n",
       "        (conv1c): Conv3d(64, 64, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "        (norm1c): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyReLU1c): LeakyReLU(negative_slope=0.01)\n",
       "        (DropOut1c): Dropout3d(p=0.05, inplace=False)\n",
       "        (conv2): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (norm2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyReLU2): LeakyReLU(negative_slope=0.01)\n",
       "        (DropOut2): Dropout3d(p=0.05, inplace=False)\n",
       "        (conv2a): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (norm2a): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyReLU2a): LeakyReLU(negative_slope=0.01)\n",
       "        (DropOut2a): Dropout3d(p=0.05, inplace=False)\n",
       "        (conv2b): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
       "        (norm2b): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyReLU2b): LeakyReLU(negative_slope=0.01)\n",
       "        (DropOut2b): Dropout3d(p=0.05, inplace=False)\n",
       "        (conv2c): Conv3d(128, 128, kernel_size=(4, 4, 4), stride=(2, 2, 2), padding=(1, 1, 1))\n",
       "        (norm2c): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (LeakyReLU2c): LeakyReLU(negative_slope=0.01)\n",
       "        (DropOut2c): Dropout3d(p=0.05, inplace=False)\n",
       "        (Flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "        (Linear): Linear(in_features=3072, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (converter): Sequential()\n",
       "  (projection_head): ProjectionHead(\n",
       "    (layers): Sequential(\n",
       "      (Linear0): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (LeakyReLU0): LeakyReLU(negative_slope=0.01)\n",
       "      (DropOut0): Dropout(p=0, inplace=False)\n",
       "      (Linear1): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (LeakyReLU1): LeakyReLU(negative_slope=0.01)\n",
       "      (DropOut1): Dropout(p=0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = model_path+\"/logs/*/version_0/checkpoints\"+r'/*.ckpt'\n",
    "files = glob.glob(paths)\n",
    "ckpt_path = files[0]\n",
    "checkpoint = torch.load(\n",
    "            ckpt_path, map_location=torch.device(config.device))\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "backbones.0.encoder.conv0.weight \t torch.Size([32, 1, 7, 7, 7])\n",
      "backbones.0.encoder.conv0.bias \t torch.Size([32])\n",
      "backbones.0.encoder.norm0.weight \t torch.Size([32])\n",
      "backbones.0.encoder.norm0.bias \t torch.Size([32])\n",
      "backbones.0.encoder.norm0.running_mean \t torch.Size([32])\n",
      "backbones.0.encoder.norm0.running_var \t torch.Size([32])\n",
      "backbones.0.encoder.norm0.num_batches_tracked \t torch.Size([])\n",
      "backbones.0.encoder.conv0a.weight \t torch.Size([32, 32, 3, 3, 3])\n",
      "backbones.0.encoder.conv0a.bias \t torch.Size([32])\n",
      "backbones.0.encoder.norm0a.weight \t torch.Size([32])\n",
      "backbones.0.encoder.norm0a.bias \t torch.Size([32])\n",
      "backbones.0.encoder.norm0a.running_mean \t torch.Size([32])\n",
      "backbones.0.encoder.norm0a.running_var \t torch.Size([32])\n",
      "backbones.0.encoder.norm0a.num_batches_tracked \t torch.Size([])\n",
      "backbones.0.encoder.conv0b.weight \t torch.Size([32, 32, 3, 3, 3])\n",
      "backbones.0.encoder.conv0b.bias \t torch.Size([32])\n",
      "backbones.0.encoder.norm0b.weight \t torch.Size([32])\n",
      "backbones.0.encoder.norm0b.bias \t torch.Size([32])\n",
      "backbones.0.encoder.norm0b.running_mean \t torch.Size([32])\n",
      "backbones.0.encoder.norm0b.running_var \t torch.Size([32])\n",
      "backbones.0.encoder.norm0b.num_batches_tracked \t torch.Size([])\n",
      "backbones.0.encoder.conv0c.weight \t torch.Size([32, 32, 4, 4, 4])\n",
      "backbones.0.encoder.conv0c.bias \t torch.Size([32])\n",
      "backbones.0.encoder.norm0c.weight \t torch.Size([32])\n",
      "backbones.0.encoder.norm0c.bias \t torch.Size([32])\n",
      "backbones.0.encoder.norm0c.running_mean \t torch.Size([32])\n",
      "backbones.0.encoder.norm0c.running_var \t torch.Size([32])\n",
      "backbones.0.encoder.norm0c.num_batches_tracked \t torch.Size([])\n",
      "backbones.0.encoder.conv1.weight \t torch.Size([64, 32, 3, 3, 3])\n",
      "backbones.0.encoder.conv1.bias \t torch.Size([64])\n",
      "backbones.0.encoder.norm1.weight \t torch.Size([64])\n",
      "backbones.0.encoder.norm1.bias \t torch.Size([64])\n",
      "backbones.0.encoder.norm1.running_mean \t torch.Size([64])\n",
      "backbones.0.encoder.norm1.running_var \t torch.Size([64])\n",
      "backbones.0.encoder.norm1.num_batches_tracked \t torch.Size([])\n",
      "backbones.0.encoder.conv1a.weight \t torch.Size([64, 64, 3, 3, 3])\n",
      "backbones.0.encoder.conv1a.bias \t torch.Size([64])\n",
      "backbones.0.encoder.norm1a.weight \t torch.Size([64])\n",
      "backbones.0.encoder.norm1a.bias \t torch.Size([64])\n",
      "backbones.0.encoder.norm1a.running_mean \t torch.Size([64])\n",
      "backbones.0.encoder.norm1a.running_var \t torch.Size([64])\n",
      "backbones.0.encoder.norm1a.num_batches_tracked \t torch.Size([])\n",
      "backbones.0.encoder.conv1b.weight \t torch.Size([64, 64, 3, 3, 3])\n",
      "backbones.0.encoder.conv1b.bias \t torch.Size([64])\n",
      "backbones.0.encoder.norm1b.weight \t torch.Size([64])\n",
      "backbones.0.encoder.norm1b.bias \t torch.Size([64])\n",
      "backbones.0.encoder.norm1b.running_mean \t torch.Size([64])\n",
      "backbones.0.encoder.norm1b.running_var \t torch.Size([64])\n",
      "backbones.0.encoder.norm1b.num_batches_tracked \t torch.Size([])\n",
      "backbones.0.encoder.conv1c.weight \t torch.Size([64, 64, 4, 4, 4])\n",
      "backbones.0.encoder.conv1c.bias \t torch.Size([64])\n",
      "backbones.0.encoder.norm1c.weight \t torch.Size([64])\n",
      "backbones.0.encoder.norm1c.bias \t torch.Size([64])\n",
      "backbones.0.encoder.norm1c.running_mean \t torch.Size([64])\n",
      "backbones.0.encoder.norm1c.running_var \t torch.Size([64])\n",
      "backbones.0.encoder.norm1c.num_batches_tracked \t torch.Size([])\n",
      "backbones.0.encoder.conv2.weight \t torch.Size([128, 64, 3, 3, 3])\n",
      "backbones.0.encoder.conv2.bias \t torch.Size([128])\n",
      "backbones.0.encoder.norm2.weight \t torch.Size([128])\n",
      "backbones.0.encoder.norm2.bias \t torch.Size([128])\n",
      "backbones.0.encoder.norm2.running_mean \t torch.Size([128])\n",
      "backbones.0.encoder.norm2.running_var \t torch.Size([128])\n",
      "backbones.0.encoder.norm2.num_batches_tracked \t torch.Size([])\n",
      "backbones.0.encoder.conv2a.weight \t torch.Size([128, 128, 3, 3, 3])\n",
      "backbones.0.encoder.conv2a.bias \t torch.Size([128])\n",
      "backbones.0.encoder.norm2a.weight \t torch.Size([128])\n",
      "backbones.0.encoder.norm2a.bias \t torch.Size([128])\n",
      "backbones.0.encoder.norm2a.running_mean \t torch.Size([128])\n",
      "backbones.0.encoder.norm2a.running_var \t torch.Size([128])\n",
      "backbones.0.encoder.norm2a.num_batches_tracked \t torch.Size([])\n",
      "backbones.0.encoder.conv2b.weight \t torch.Size([128, 128, 3, 3, 3])\n",
      "backbones.0.encoder.conv2b.bias \t torch.Size([128])\n",
      "backbones.0.encoder.norm2b.weight \t torch.Size([128])\n",
      "backbones.0.encoder.norm2b.bias \t torch.Size([128])\n",
      "backbones.0.encoder.norm2b.running_mean \t torch.Size([128])\n",
      "backbones.0.encoder.norm2b.running_var \t torch.Size([128])\n",
      "backbones.0.encoder.norm2b.num_batches_tracked \t torch.Size([])\n",
      "backbones.0.encoder.conv2c.weight \t torch.Size([128, 128, 4, 4, 4])\n",
      "backbones.0.encoder.conv2c.bias \t torch.Size([128])\n",
      "backbones.0.encoder.norm2c.weight \t torch.Size([128])\n",
      "backbones.0.encoder.norm2c.bias \t torch.Size([128])\n",
      "backbones.0.encoder.norm2c.running_mean \t torch.Size([128])\n",
      "backbones.0.encoder.norm2c.running_var \t torch.Size([128])\n",
      "backbones.0.encoder.norm2c.num_batches_tracked \t torch.Size([])\n",
      "backbones.0.encoder.Linear.weight \t torch.Size([256, 3072])\n",
      "backbones.0.encoder.Linear.bias \t torch.Size([256])\n",
      "projection_head.layers.Linear0.weight \t torch.Size([256, 256])\n",
      "projection_head.layers.Linear0.bias \t torch.Size([256])\n",
      "projection_head.layers.Linear1.weight \t torch.Size([256, 256])\n",
      "projection_head.layers.Linear1.bias \t torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visu kernels #is it the right axis ??\n",
    "kernels = model.state_dict()['backbones.0.encoder.conv0.weight'].detach()\n",
    "for i in range(32):\n",
    "    for k in range(7):\n",
    "        kernel = kernels[i,0,k,:,:]\n",
    "        plt.imshow(kernel)\n",
    "        plt.savefig(f'/home/jl277509/Documents/plot_kernels/kernel_{i}_slice{k}.png')\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image\n",
    "skeletons = np.load('/neurospin/dico/data/deep_folding/current/datasets/hcp/crops/2mm/ORBITAL/mask/Lskeleton.npy')\n",
    "skeletons = skeletons != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find a good subject, with an interruption, visible in a plane ?\n",
    "# first save the slices of this subject ? Or add to the subplot ?\n",
    "subjects = pd.read_csv('/neurospin/dico/data/deep_folding/current/datasets/hcp/crops/2mm/ORBITAL/mask/Lskeleton_subject.csv')\n",
    "id = 107018\n",
    "id = 104820\n",
    "id = 116524\n",
    "idx = subjects.loc[subjects['Subject']==id].index.tolist()[0]\n",
    "data = skeletons[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1, 30, 38, 22])\n"
     ]
    }
   ],
   "source": [
    "data = np.transpose(data, [3, 0, 1, 2])\n",
    "data[data > 0] = 1\n",
    "data = data.astype(np.float32)\n",
    "data = torch.from_numpy(data)\n",
    "data.unsqueeze_(0)\n",
    "data.unsqueeze_(0)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.conv1 doesn't exist !!\n",
    "# model.backbones[0].encoder.conv0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For given conv layer, visu filters and voxel clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = 'conv0'\n",
    "# NB: also need to change conv1 below ...\n",
    "step_data = 1 # because of stride # conv0: 1, conv1: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature maps\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "model.backbones[0].encoder.conv0.register_forward_hook(get_activation(conv))\n",
    "output = model(data)\n",
    "\n",
    "act = activation[conv].squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jl277509/Documents/plot_conv/116524/conv0/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[311], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m save_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/jl277509/Documents/plot_conv/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(save_dir):\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39msavefig(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_slice_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jl277509/Documents/plot_conv/116524/conv0/'"
     ]
    }
   ],
   "source": [
    "nrows=4\n",
    "for k in range(0, act.shape[-1]):\n",
    "    fig, axs = plt.subplots(figsize=(((act.shape[0])//nrows)*4, 4*4), ncols=(act.shape[0])//nrows, nrows=nrows+1)\n",
    "    for idx in range(act.shape[0]):\n",
    "        axs[(idx)%nrows, (idx)//nrows].imshow(act[idx, :,:,k])\n",
    "    axs[(act.shape[0]-1)%nrows+1, (act.shape[0]-1)//nrows].imshow(data[0,0,0,:,:,k*step_data])\n",
    "    save_dir = f'/home/jl277509/Documents/plot_conv/{str(id)}/{conv}/'\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    plt.savefig(os.path.join(save_dir, f'{conv}_slice_{k}.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voxel clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25080, 32)\n",
      "25080\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7], dtype=int32), array([ 1875, 17387,   875,   408,   893,   723,  1862,  1057]))\n",
      "(25080, 32)\n",
      "(1, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "n_clusters=8\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0, n_init=1)\n",
    "dim_x, dim_y, dim_z = act[0].shape\n",
    "\n",
    "coords_list = []\n",
    "reshaped_f_map = []\n",
    "for x in range(dim_x):\n",
    "    for y in range(dim_y):\n",
    "        for z in range(dim_z):\n",
    "            vector = act[:,x,y,z]\n",
    "            reshaped_f_map.append(vector)\n",
    "            # keep coords for reshape\n",
    "            coords = (x,y,z)\n",
    "            coords_list.append(coords)\n",
    "\n",
    "reshaped_f_map = np.array(reshaped_f_map)\n",
    "print(reshaped_f_map.shape)\n",
    "\n",
    "clusters = kmeans.fit_predict(reshaped_f_map)\n",
    "print(len(clusters))\n",
    "print(np.unique(clusters, return_counts=True))\n",
    "\n",
    "dim_x, dim_y, dim_z = act[0].shape\n",
    "\n",
    "coords_list = []\n",
    "reshaped_f_map = []\n",
    "for x in range(dim_x):\n",
    "    for y in range(dim_y):\n",
    "        for z in range(dim_z):\n",
    "            vector = act[:,x,y,z]\n",
    "            reshaped_f_map.append(vector)\n",
    "            # keep coords for reshape\n",
    "            coords = (x,y,z)\n",
    "            coords_list.append(coords)\n",
    "\n",
    "reshaped_f_map = np.array(reshaped_f_map)\n",
    "print(reshaped_f_map.shape)\n",
    "\n",
    "clusters = kmeans.fit_predict(reshaped_f_map)\n",
    "\n",
    "# Reshape to image\n",
    "cluster_matrix = np.zeros((dim_x, dim_y, dim_z))\n",
    "for coords, elem in zip(coords_list, clusters):\n",
    "    cluster_matrix[coords]=elem\n",
    "\n",
    "# zoom to scale the deep layers to original image shape\n",
    "ratio= (skeletons.shape[1]//act.shape[1], skeletons.shape[2]//act.shape[2], skeletons.shape[3]//act.shape[3])\n",
    "print(ratio)\n",
    "clust = zoom(cluster_matrix, ratio, order=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save matrix as nifti for 2D visu with anatomist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = cluster_matrix.astype(np.int16)\n",
    "arr = np.array(arr, order='F')\n",
    "vol = aims.Volume(arr)\n",
    "vol.header()['voxel_size'] = [2, 2, 2]\n",
    "aims.write(vol, f'/neurospin/dico/jlaval/data/clusters_visu/{str(id)}_{n_clusters}clusters_{conv}.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save matrix for each cluster, then make a bucket for visu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(n_clusters):\n",
    "    single_cluster = cluster_matrix == k\n",
    "    arr = single_cluster.astype(bool)\n",
    "    arr = np.array(arr, order='F')\n",
    "    vol = aims.Volume(arr)\n",
    "    vol.header()['voxel_size'] = [2, 2, 2]\n",
    "    aims.write(vol, f'/neurospin/dico/jlaval/data/clusters_visu/binary/{str(id)}_{n_clusters}clusters_{conv}.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save cluster slices as png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: need to add a fixed colorbar across the slices !!!\n",
    "for k in range(cluster_matrix.shape[-1]):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(cluster_matrix[:,:,k])\n",
    "    save_dir = f'/home/jl277509/Documents/plot_clusters/{str(id)}/{conv}/'\n",
    "    if not os.path.isdir(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    plt.savefig(os.path.join(save_dir, f'{conv}_{n_clusters}clusters_slice_{k}.png'))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize conv filter # need to adapt to 3D\n",
    "kernels = model.backbones[0].encoder.conv0.weight.detach()\n",
    "fig, axarr = plt.subplots(kernels.size(0))\n",
    "for idx in range(kernels.size(0)):\n",
    "    axarr[idx].imshow(kernels[idx].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
