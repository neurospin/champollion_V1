{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "\n",
    "#savedir = '/neurospin/dico/data/deep_folding/current/models/Champollion_V0_trained_on_UKB40/'\n",
    "#savedir = '/neurospin/dico/data/deep_folding/current/models/Champollion_V1_after_ablation_latent_256/'\n",
    "savedir = '/neurospin/dico/data/deep_folding/current/models/Champollion_V1_after_ablation/'\n",
    "#dfs_dir = '/neurospin/dico/data/deep_folding/current/models/Champollion_V0_trained_on_UKB40/embeddings/ukb40_epoch80_embeddings'\n",
    "#dfs_dir = '/neurospin/dico/data/deep_folding/current/models/Champollion_V1_after_ablation_latent_256/embeddings/ukb40_epoch80_embeddings'\n",
    "n_dims = 32\n",
    "\n",
    "regions_to_treat = ['SOr_left',\n",
    " 'SOr_right',\n",
    " 'FColl-SRh_left',\n",
    " 'SFmedian-SFpoltr-SFsup_left',\n",
    " 'SFinf-BROCA-SPeCinf_left',\n",
    " 'SPoC_left',\n",
    " 'fronto-parietal_medial_face_left',\n",
    " 'FIP_left',\n",
    " 'CINGULATE_left',\n",
    " 'SC-SPoC_left',\n",
    " 'SFinter-SFsup_left',\n",
    " 'FCMpost-SpC_left',\n",
    " 'SsP-SPaint_left',\n",
    " 'SOr-SOlf_left',\n",
    " 'FPO-SCu-ScCal_left',\n",
    " 'SFmarginal-SFinfant_left',\n",
    " 'SFint-FCMant_left',\n",
    " 'STi-STs-STpol_left',\n",
    " 'SFint-SR_left',\n",
    " 'Lobule_parietal_sup_left',\n",
    " 'STi-SOTlat_left',\n",
    " 'SPeC_left',\n",
    " 'STsbr_left',\n",
    " 'ScCal-SLi_left',\n",
    " 'STs_left',\n",
    " 'FCLp-subsc-FCLa-INSULA_left',\n",
    " 'SC-sylv_left',\n",
    " 'SC-SPeC_left',\n",
    " 'OCCIPITAL_left',\n",
    " 'FColl-SRh_right',\n",
    " 'SFmedian-SFpoltr-SFsup_right',\n",
    " 'SFinf-BROCA-SPeCinf_right',\n",
    " 'SPoC_right',\n",
    " 'fronto-parietal_medial_face_right',\n",
    " 'FIP_right',\n",
    " 'CINGULATE_right',\n",
    " 'SC-SPoC_right',\n",
    " 'SFinter-SFsup_right',\n",
    " 'FCMpost-SpC_right',\n",
    " 'SsP-SPaint_right',\n",
    " 'SOr-SOlf_right',\n",
    " 'FPO-SCu-ScCal_right',\n",
    " 'SFmarginal-SFinfant_right',\n",
    " 'SFint-FCMant_right',\n",
    " 'STi-STs-STpol_right',\n",
    " 'SFint-SR_right',\n",
    " 'Lobule_parietal_sup_right',\n",
    " 'STi-SOTlat_right',\n",
    " 'SPeC_right',\n",
    " 'STsbr_right',\n",
    " 'ScCal-SLi_right',\n",
    " 'STs_right',\n",
    " 'FCLp-subsc-FCLa-INSULA_right',\n",
    " 'SC-sylv_right',\n",
    " 'SC-SPeC_right',\n",
    " 'OCCIPITAL_right']\n",
    "\n",
    "## NB : add LARGE_CINGULATE ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb train val : 1044, nb test : 118, nb test extra : 130\n",
      "(array(['F', 'M', 'bd', 'control', 'fep', 'psychotic bd',\n",
      "       'relative of bipolar disorder',\n",
      "       'relative of schizoaffective disorder',\n",
      "       'relative of schizophrenia', 'schizoaffective disorder', 'scz'],\n",
      "      dtype=object), array([117, 155,  35, 667,  43, 140, 122, 134, 176, 124, 491]))\n",
      "nb labels : 1282\n"
     ]
    }
   ],
   "source": [
    "# select the train val subjects\n",
    "dataset = 'schiz'\n",
    "all_subjects = pd.read_csv('/neurospin/dico/data/deep_folding/current/datasets/aggregate_schizophrenia/all_subjects.csv')\n",
    "all_subjects.columns = ['ID']\n",
    "splits_dir = '/neurospin/dico/data/deep_folding/current/datasets/aggregate_schizophrenia/splits/'\n",
    "train_val_subjects_dirs = glob.glob(f'{splits_dir}/train_val_*')\n",
    "# load each split subject file and create cv splits from them\n",
    "train_val_subjects = []\n",
    "for i, directory in enumerate(train_val_subjects_dirs):\n",
    "    train_val_subjects.append(pd.read_csv(directory, sep='\\t', header=None))\n",
    "train_val_subjects = pd.concat(train_val_subjects, axis=0)\n",
    "train_val_subjects.columns = ['ID']\n",
    "train_val_subjects['ID'] = train_val_subjects['ID'].astype(str)\n",
    "\n",
    "# select the test subjects\n",
    "test_subjects = pd.read_csv(f'{splits_dir}/internal_test.csv', header=None)\n",
    "test_subjects.columns = ['ID']\n",
    "test_subjects['ID'] = test_subjects['ID'].astype(str)\n",
    "# select the test extra\n",
    "test_extra_subjects = pd.read_csv(f'{splits_dir}/external_test.csv', header=None)\n",
    "test_extra_subjects.columns = ['ID']\n",
    "test_extra_subjects['ID'] = test_extra_subjects['ID'].astype(str)\n",
    "\n",
    "# print lengths\n",
    "print(f'nb train val : {len(train_val_subjects)}, nb test : {len(test_subjects)}, nb test extra : {len(test_extra_subjects)}')\n",
    "\n",
    "# load labels\n",
    "labels1 = pd.read_csv('/neurospin/psy/schizconnect-vip-prague/participants_v-20231108.tsv', usecols=['participant_id', 'diagnosis', 'sex', 'age', 'site'], sep='\\t')\n",
    "labels2 = pd.read_csv('/neurospin/psy/bsnip1/participants_v-20231108.tsv', usecols=['participant_id', 'diagnosis', 'sex', 'age', 'site'], sep='\\t')\n",
    "labels3 = pd.read_csv('/neurospin/psy/candi/participants_v-20231108.tsv', usecols=['participant_id', 'diagnosis', 'sex', 'age'], sep='\\t')\n",
    "labels3['site'] = ['CANDI'] * len(labels3)\n",
    "labels4 = pd.read_csv('/neurospin/psy/cnp/participants_v-20231108.tsv', usecols=['participant_id', 'diagnosis', 'sex', 'age', 'ScannerSerialNumber'], sep='\\t')\n",
    "labels4['ScannerSerialNumber'] = labels4['ScannerSerialNumber'].apply(lambda x: str(int(x)))\n",
    "labels4.columns=['participant_id', 'diagnosis', 'sex', 'age', 'site']\n",
    "labels = pd.concat([labels1, labels2, labels3, labels4], axis=0)\n",
    "labels.columns = ['ID'] + labels.columns[1:].tolist()\n",
    "labels.dropna()\n",
    "label = 'diagnosis'\n",
    "pathology = 'schizophrenia'\n",
    "# not only bipolar\n",
    "print(np.unique(labels['diagnosis'], return_counts=True))\n",
    "# drop rows for which diagnosis is not control or pathology\n",
    "labels = labels[labels[label].isin(['control', 'schizoaffective disorder', 'scz'])] # no site info for schizo affective disorder ?\n",
    "labels['ID'] = labels['ID'].astype(str)\n",
    "print(f'nb labels : {len(labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to make correspondance between IDs\n",
    "def make_correspondance_ids(df1, df2, colname='ID2'):\n",
    "    \"\"\"\n",
    "    Make correspondance between IDs. Adds column ID2 to df1.\n",
    "    \"\"\"\n",
    "\n",
    "    new_df = df1.copy()\n",
    "\n",
    "    id2_dict = {}\n",
    "    for id1 in df1['ID'].values:\n",
    "        filtered_df2 = df2[df2['ID'].apply(lambda x: (x in id1) or (id1 in x))]\n",
    "        id2 = filtered_df2['ID'].tolist()\n",
    "        if len(id2) > 0:\n",
    "            id2 = id2[0]\n",
    "            id2_dict[id1] = id2\n",
    "    new_df[colname] = new_df['ID'].map(id2_dict)\n",
    "    new_df = new_df.dropna()\n",
    "    return new_df   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1292\n"
     ]
    }
   ],
   "source": [
    "## group the train val and test subjects + TEST EXTRA\n",
    "train_val_test_test_extra_subjects = pd.concat([train_val_subjects, test_subjects, test_extra_subjects], axis=0)\n",
    "train_val_test_test_extra_subjects['ID'] = train_val_test_test_extra_subjects['ID'].astype(str)\n",
    "print(len(train_val_test_test_extra_subjects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge train_val_test and labels\n",
    "df = make_correspondance_ids(labels, train_val_test_test_extra_subjects, colname='ID_train_val_test')\n",
    "# merge df and all_subjects\n",
    "df = make_correspondance_ids(df, all_subjects, colname='ID_all_subjects')\n",
    "# drop\n",
    "df = df.drop_duplicates()\n",
    "df = df.dropna()\n",
    "df.columns = ['ID_labels', 'sex', 'age', 'diagnosis', 'site', 'ID_train_val_test', 'ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## REMOVE PRAGUE, CONTAINS CONTROLS ONLY\n",
    "df = df.loc[df['site']!='PRAGUE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['_left', '_right']\n",
    "all_matches = []\n",
    "for var in keywords:\n",
    "    pattern = f\"{savedir}*{var}/*/{dataset}_random_embeddings/full_embeddings.csv\" ## TODO : Make sure only one model per region, take highest epoch, and print\n",
    "    matches = glob.glob(pattern)\n",
    "    all_matches.extend(matches)\n",
    "# Optional: remove duplicates\n",
    "dfs_dirs = list(set(all_matches))\n",
    "dfs_dirs.sort()\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 3/56 [00:00<00:02, 22.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treating CINGULATE_left\n",
      "Length embeddings :  1029\n",
      "Treating CINGULATE_right\n",
      "Length embeddings :  1029\n",
      "Treating FCLp-subsc-FCLa-INSULA_left\n",
      "Length embeddings :  1029\n",
      "Treating FCLp-subsc-FCLa-INSULA_right\n",
      "Length embeddings :  1029\n",
      "Treating FCMpost-SpC_left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 6/56 [00:00<00:03, 12.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length embeddings :  1029\n",
      "Treating FCMpost-SpC_right\n",
      "Length embeddings :  1029\n",
      "Treating FColl-SRh_left\n",
      "Length embeddings :  1029\n",
      "Treating FColl-SRh_right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 8/56 [00:00<00:05,  8.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length embeddings :  1029\n",
      "Treating FIP_left\n",
      "Length embeddings :  1029\n",
      "Treating FIP_right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 12/56 [00:01<00:04,  9.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length embeddings :  1029\n",
      "Treating FPO-SCu-ScCal_left\n",
      "Length embeddings :  1029\n",
      "Treating FPO-SCu-ScCal_right\n",
      "Length embeddings :  1029\n",
      "Treating Lobule_parietal_sup_left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 14/56 [00:01<00:04,  9.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length embeddings :  1029\n",
      "Treating Lobule_parietal_sup_right\n",
      "Length embeddings :  1029\n",
      "Treating OCCIPITAL_left\n",
      "Length embeddings :  1029\n",
      "Treating OCCIPITAL_right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 18/56 [00:01<00:03, 11.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length embeddings :  1029\n",
      "Treating SC-SPeC_left\n",
      "Length embeddings :  1029\n",
      "Treating SC-SPeC_right\n",
      "Length embeddings :  1029\n",
      "Treating SC-SPoC_left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 24/56 [00:01<00:01, 18.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length embeddings :  1029\n",
      "Treating SC-SPoC_right\n",
      "Length embeddings :  1029\n",
      "Treating SC-sylv_left\n",
      "Length embeddings :  1029\n",
      "Treating SC-sylv_right\n",
      "Length embeddings :  1029\n",
      "Treating SFinf-BROCA-SPeCinf_left\n",
      "Length embeddings :  1029\n",
      "Treating SFinf-BROCA-SPeCinf_right\n",
      "Length embeddings :  1029\n",
      "Treating SFint-FCMant_left\n",
      "Length embeddings :  1029\n",
      "Treating SFint-FCMant_right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 27/56 [00:02<00:01, 18.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length embeddings :  1029\n",
      "Treating SFint-SR_left\n",
      "Length embeddings :  1029\n",
      "Treating SFint-SR_right\n",
      "Length embeddings :  1029\n",
      "Treating SFinter-SFsup_left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 31/56 [00:02<00:01, 15.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length embeddings :  1029\n",
      "Treating SFinter-SFsup_right\n",
      "Length embeddings :  1029\n",
      "Treating SFmarginal-SFinfant_left\n",
      "Length embeddings :  1029\n",
      "Treating SFmarginal-SFinfant_right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 34/56 [00:02<00:01, 16.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length embeddings :  1029\n",
      "Treating SFmedian-SFpoltr-SFsup_left\n",
      "Length embeddings :  1029\n",
      "Treating SFmedian-SFpoltr-SFsup_right\n",
      "Length embeddings :  1029\n",
      "Treating SOr-SOlf_left\n",
      "Length embeddings :  1029\n",
      "Treating SOr-SOlf_right\n",
      "Length embeddings :  1029\n",
      "Treating SOr_left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 38/56 [00:02<00:00, 18.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length embeddings :  1029\n",
      "Treating SOr_right\n",
      "Length embeddings :  1029\n",
      "Treating SPeC_left\n",
      "Length embeddings :  1029\n",
      "Treating SPeC_right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 42/56 [00:02<00:00, 17.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length embeddings :  1029\n",
      "Treating SPoC_left\n",
      "Length embeddings :  1029\n",
      "Treating SPoC_right\n",
      "Length embeddings :  1029\n",
      "Treating STi-SOTlat_left\n",
      "Length embeddings :  1029\n",
      "Treating STi-SOTlat_right\n",
      "Length embeddings :  1029\n",
      "Treating STi-STs-STpol_left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 45/56 [00:03<00:00, 19.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length embeddings :  1029\n",
      "Treating STi-STs-STpol_right\n",
      "Length embeddings :  1029\n",
      "Treating STs_left\n",
      "Length embeddings :  1029\n",
      "Treating STs_right\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 50/56 [00:03<00:00, 13.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length embeddings :  1029\n",
      "Treating STsbr_left\n",
      "Length embeddings :  1029\n",
      "Treating STsbr_right\n",
      "Length embeddings :  1029\n",
      "Treating ScCal-SLi_left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 52/56 [00:03<00:00, 11.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length embeddings :  1029\n",
      "Treating ScCal-SLi_right\n",
      "Length embeddings :  1029\n",
      "Treating SsP-SPaint_left\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [00:04<00:00, 13.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length embeddings :  1029\n",
      "Treating SsP-SPaint_right\n",
      "Length embeddings :  1029\n",
      "Treating fronto-parietal_medial_face_left\n",
      "Length embeddings :  1029\n",
      "Treating fronto-parietal_medial_face_right\n",
      "Length embeddings :  1029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## iterate on the regions\n",
    "embds_dict = {}\n",
    "\n",
    "p_values = {f'Split{i}': {} for i in range(2)}\n",
    "r2_values = {f'Split{i}': {} for i in range(2)}\n",
    "preds = {f'Split{i}': {} for i in range(2)}\n",
    "\n",
    "for i, directory in enumerate(tqdm(dfs_dirs)):\n",
    "    region = directory.split('/')[-4]\n",
    "    print(f'Treating {region}')\n",
    "    embd=pd.read_csv(directory)\n",
    "    embd=pd.merge(embd, df, on='ID')\n",
    "    \n",
    "    # fit standard scaler on embd and transform\n",
    "    std = StandardScaler()\n",
    "    embd_matrix = embd.loc[:, embd.columns.str.startswith('dim')]\n",
    "    std_embds = std.fit_transform(embd_matrix)\n",
    "    embd.loc[:, embd.columns.str.startswith('dim')] = std_embds\n",
    "    print('Length embeddings : ', embd.shape[0])\n",
    "    embds_dict[region]=np.array(embd_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# concat the embeddings to get N regions * latent size input tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack([embds_dict[reg] for reg in regions_to_treat], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = embd['diagnosis'].tolist()\n",
    "Y = np.array([1 if diag!='control' else 0 for diag in Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train / val / test\n",
    "train_idxs = embd.loc[embd['ID_train_val_test'].isin(train_val_subjects['ID'])].index.to_numpy().astype(int)\n",
    "test_idxs = embd.loc[embd['ID_train_val_test'].isin(test_subjects['ID'])].index.to_numpy().astype(int)\n",
    "X_train, Y_train = X[train_idxs], Y[train_idxs]\n",
    "X_test, Y_test = X[test_idxs], Y[test_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((804, 32, 56), (95, 32, 56))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non Linear / Linear classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomConcatClassifier(nn.Module):\n",
    "    def __init__(self, N, d, p):\n",
    "        super(CustomConcatClassifier, self).__init__()\n",
    "        self.N = N\n",
    "        self.d = d\n",
    "        self.p = p\n",
    "\n",
    "        # One unique linear layer per input vector\n",
    "        self.individual_linears = nn.ModuleList([nn.Linear(d, p) for _ in range(N)])\n",
    "\n",
    "        # activation\n",
    "        self.activation = nn.LeakyReLU()\n",
    "\n",
    "        # Final classifier: from concatenated vector (N*p) to 1\n",
    "        self.classifier = nn.Linear(N * p, 1)\n",
    "\n",
    "        self.m = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch_size, N, d)\n",
    "        \"\"\"\n",
    "        assert x.shape[1:] == (self.d, self.N), f\"Expected input shape (batch_size, {self.d}, {self.N})\"\n",
    "\n",
    "        # Apply each individual linear to its corresponding vector\n",
    "        features = []\n",
    "        for i in range(self.N):\n",
    "            vec = x[:, :, i]                     # shape: (batch_size, d)\n",
    "            out = self.individual_linears[i](vec)  # shape: (batch_size, p)\n",
    "            features.append(out)\n",
    "\n",
    "        # Concatenate all transformed vectors: shape (batch_size, N*p)\n",
    "        concatenated = torch.cat(features, dim=1)\n",
    "\n",
    "        # activation\n",
    "        concatenated = self.activation(concatenated)\n",
    "\n",
    "        # Final classification layer\n",
    "        logits = self.classifier(concatenated)   # shape: (batch_size, 1)\n",
    "\n",
    "        # Binary classification using softmax over 2 outputs: [-logit, +logit]\n",
    "        #probs = F.softmax(torch.cat([-logits, logits], dim=1), dim=1)  # shape: (batch_size, 2)\n",
    "        probs = self.m(logits)\n",
    "\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your data is in NumPy arrays\n",
    "X_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y_train, dtype=torch.float32)\n",
    "from torch.utils.data import TensorDataset\n",
    "train_dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "from torch.utils.data import DataLoader\n",
    "trainloader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y_test, dtype=torch.float32)\n",
    "test_dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "testloader = DataLoader(test_dataset, batch_size=1, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomConcatClassifier(\n",
       "  (individual_linears): ModuleList(\n",
       "    (0-55): 56 x Linear(in_features=32, out_features=3, bias=True)\n",
       "  )\n",
       "  (activation): LeakyReLU(negative_slope=0.01)\n",
       "  (classifier): Linear(in_features=168, out_features=1, bias=True)\n",
       "  (m): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomConcatClassifier(N=56, d=32, p=3)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(804, 32, 56)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:00<00:23,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(67.5984, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [00:01<00:21,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(39.0828, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [00:02<00:20,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.0276, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [00:02<00:18,  1.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20.9990, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [00:03<00:17,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(16.0949, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [00:04<00:16,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13.2307, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [00:04<00:14,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.4017, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [00:05<00:12,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(10.2459, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [00:05<00:11,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.7094, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [00:06<00:10,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.4718, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [00:06<00:10,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.4544, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [00:07<00:10,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.5185, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [00:08<00:10,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.2930, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [00:08<00:09,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.7016, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [00:09<00:09,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.2804, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [00:09<00:08,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.6443, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [00:10<00:07,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.7764, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [00:11<00:07,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.2513, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [00:11<00:06,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.7947, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [00:12<00:05,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.5120, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [00:12<00:05,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4787, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [00:13<00:04,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8829, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [00:14<00:04,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6961, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [00:14<00:03,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4959, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [00:15<00:02,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4244, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [00:15<00:02,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3354, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [00:16<00:01,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1492, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [00:17<00:01,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0306, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [00:17<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9331, device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:18<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8665, device='cuda:0', grad_fn=<AddBackward0>)\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(10)):  # loop over the dataset multiple times\n",
    "\n",
    "    loss_epoch = 0\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.reshape(-1), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_epoch += loss\n",
    "    \n",
    "    print(loss_epoch)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomConcatClassifier(\n",
       "  (individual_linears): ModuleList(\n",
       "    (0-55): 56 x Linear(in_features=32, out_features=3, bias=True)\n",
       "  )\n",
       "  (activation): LeakyReLU(negative_slope=0.01)\n",
       "  (classifier): Linear(in_features=168, out_features=1, bias=True)\n",
       "  (m): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = []\n",
    "for data in testloader:\n",
    "    inputs, _ = data\n",
    "    inputs = inputs.to(device)\n",
    "    outputs = model(inputs)\n",
    "    out = outputs.detach().cpu().numpy()\n",
    "    outs.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.616279069767442"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc = roc_auc_score(Y_test.reshape(-1,1), np.array(outs).reshape(-1,1))\n",
    "roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
