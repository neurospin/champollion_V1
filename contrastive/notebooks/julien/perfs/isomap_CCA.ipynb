{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.cross_decomposition import CCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embs = pd.read_csv('/neurospin/dico/jlaval/Output/SC-sylv_12-16/16-38-32_170/troiani_custom_embeddings/custom_cross_val_embeddings.csv')\n",
    "#embs = pd.read_csv('/neurospin/dico/jlaval/Output/SC-sylv_12-16/16-39-27_30/troiani_custom_embeddings/custom_cross_val_embeddings.csv')\n",
    "#embs = pd.read_csv('/neurospin/dico/jlaval/Output/SC-sylv_left_v1/keep_bottom/troiani_custom_embeddings/custom_cross_val_embeddings.csv') ## V1 before even changing cutin prop\n",
    "#embs = pd.read_csv('/neurospin/dico/jlaval/Output/SC-sylv_left_v1/no_keep_bottom/troiani_custom_embeddings/custom_cross_val_embeddings.csv') ## same here = best ?\n",
    "#embs = pd.read_csv('/volatile/jl277509/Runs/02_STS_babies/Program/Output/SC-sylv_isomaps/16-40-54_148/troiani_embeddings/custom_cross_val_embeddings.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score : 0.1791501817258981\n",
      "Average correlation : 0.9552178807047264\n",
      "R2 score : 0.16115951622098967\n",
      "Average correlation : 0.9575168623723367\n",
      "R2 score : 0.1760009660158106\n",
      "Average correlation : 0.9570704518519696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/neurospin/dico/jlaval/Runs_jl277509/2023_jlaval_STSbabies/venv/lib/python3.10/site-packages/sklearn/cross_decomposition/_pls.py:113: ConvergenceWarning: Maximum number of iterations reached\n",
      "  warnings.warn(\"Maximum number of iterations reached\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score : 0.14660926525568807\n",
      "Average correlation : 0.9559917160472278\n",
      "R2 score : 0.16832458886068602\n",
      "Average correlation : 0.9564313873421826\n"
     ]
    }
   ],
   "source": [
    "## V0 with 256 dim\n",
    "for k in range(5):\n",
    "    embs = pd.read_csv(f'/volatile/jl277509/Runs/02_STS_babies/Program/Output/ISOMAP_CENTRAL/22-58-04_{k}/troiani_custom_embeddings/custom_cross_val_embeddings.csv')\n",
    "    embs.columns=['Subject']+embs.columns[1:].tolist()\n",
    "    embs_orig = embs.drop_duplicates(subset='Subject', keep='first')\n",
    "    cols_embs = [f'dim{k}' for k in range(1,257)]\n",
    "\n",
    "    # reformat embeddings and isomap\n",
    "    isomap = pd.read_csv('/neurospin/dico/data/deep_folding/current/datasets/hcp/hcp_isomap_labels.csv')\n",
    "    merged = pd.merge(isomap, embs)\n",
    "    cols_iso = [f'Isomap_central_left_dim{k}' for k in range(1,7)]\n",
    "    embs, isomap = merged[cols_embs].to_numpy(), merged[cols_iso].to_numpy()\n",
    "\n",
    "    # fit CCA and return score\n",
    "    cca = CCA(n_components=len(cols_iso), scale=False) ## need to set scale to false !!\n",
    "    cca.fit(embs, isomap)\n",
    "    trans_embs, trans_isomap = cca.transform(embs, isomap)\n",
    "    # Compute the canonical correlations between the canonical variables\n",
    "    canonical_correlations = np.corrcoef(trans_embs.T, trans_isomap.T)[:trans_embs.shape[1], trans_isomap.shape[1]:]\n",
    "    # Get the average of the canonical correlations\n",
    "    average_correlation = np.mean(np.diagonal(canonical_correlations))\n",
    "\n",
    "    # default sklearn score\n",
    "    print(f'R2 score : {cca.score(embs, isomap)}')\n",
    "    # custom score\n",
    "    print(f'Average correlation : {average_correlation}')\n",
    "\n",
    "    ## WHY WOULD CCA BE MUCH WORSE IF EACH R2 IS SIMILAR ??\n",
    "    ## Not regularized, but I don't divide in train test...\n",
    "    # should I split as train / test ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-51.72307740713265\n"
     ]
    }
   ],
   "source": [
    "## V0 with 32 dim PCA\n",
    "embs = pd.read_csv('/neurospin/dico/data/deep_folding/current/models/Champollion_V0/SC-sylv_left/11-43-38_2/hcp_random_embeddings/full_embeddings.csv')\n",
    "embs.columns=['Subject']+embs.columns[1:].tolist()\n",
    "embs_orig = embs.drop_duplicates(subset='Subject', keep='first')\n",
    "cols_orig = [f'dim{k}' for k in range(1,257)]\n",
    "cols_embs = [f'dim{k}' for k in range(1,33)]\n",
    "\n",
    "# perform PCA on the embeddings to keep 32 dims\n",
    "pca = PCA(n_components=32)\n",
    "embs = pca.fit_transform(embs_orig[cols_orig])\n",
    "embs = pd.DataFrame(columns=cols_embs, data=embs)\n",
    "embs['Subject']=embs_orig['Subject']\n",
    "\n",
    "# reformat embeddings and isomap\n",
    "isomap = pd.read_csv('/neurospin/dico/data/deep_folding/current/datasets/hcp/hcp_isomap_labels.csv')\n",
    "merged = pd.merge(isomap, embs)\n",
    "cols_iso = [f'Isomap_central_left_dim{k}' for k in range(1,7)]\n",
    "embs, isomap = merged[cols_embs].to_numpy(), merged[cols_iso].to_numpy()\n",
    "\n",
    "# fit CCA and return score\n",
    "cca = CCA(n_components=len(cols_iso), scale=False) ## need to set scale to false !!\n",
    "cca.fit(embs, isomap)\n",
    "projected_embs = cca.transform(embs)\n",
    "print(cca.score(embs, isomap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score : 0.1264129996679895\n",
      "Average correlation : 0.6281532014674703\n",
      "R2 score : 0.3732871000057694\n",
      "Average correlation : 0.7442689941755797\n",
      "R2 score : 0.42213828437062256\n",
      "Average correlation : 0.7299465930197826\n",
      "R2 score : 0.40206606449769183\n",
      "Average correlation : 0.7285365769971334\n"
     ]
    }
   ],
   "source": [
    "# V1\n",
    "\n",
    "model = 'all_augms_1'\n",
    "epochs = [0,10,20,30]\n",
    "\n",
    "for epoch in epochs:\n",
    "    embs = pd.read_csv(f'/neurospin/dico/jlaval/Output/SC-sylv_left/{model}/hcp_isomap_random_epoch{epoch}_embeddings/full_embeddings.csv')\n",
    "\n",
    "    # reformat embeddings and isomap\n",
    "    isomap = pd.read_csv('/neurospin/dico/data/deep_folding/current/datasets/hcp/hcp_isomap_labels.csv')\n",
    "    embs.columns=['Subject']+embs.columns[1:].tolist()\n",
    "    merged = pd.merge(isomap, embs)\n",
    "    cols_embs = [f'dim{k}' for k in range(1,33)]\n",
    "    cols_iso = [f'Isomap_central_left_dim{k}' for k in range(1,7)]\n",
    "    embs, isomap = merged[cols_embs].to_numpy(), merged[cols_iso].to_numpy()\n",
    "\n",
    "    # fit CCA and return score\n",
    "    cca = CCA(n_components=len(cols_iso), scale=False) ## need to set scale to false !!\n",
    "    cca.fit(embs, isomap)\n",
    "    trans_embs, trans_isomap = cca.transform(embs, isomap)\n",
    "    # Compute the canonical correlations between the canonical variables\n",
    "    canonical_correlations = np.corrcoef(trans_embs.T, trans_isomap.T)[:trans_embs.shape[1], trans_isomap.shape[1]:]\n",
    "    # Get the average of the canonical correlations\n",
    "    average_correlation = np.mean(np.diagonal(canonical_correlations))\n",
    "\n",
    "    # default sklearn score\n",
    "    print(f'R2 score : {cca.score(embs, isomap)}')\n",
    "    # custom score\n",
    "    print(f'Average correlation : {average_correlation}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need a train / test to account for overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score : 0.28638116749253384\n",
      "Average correlation : 0.6527788790051248\n",
      "Individual scores: [0.79685426 0.82249344 0.70803855 0.62203189 0.54318309 0.42407206]\n"
     ]
    }
   ],
   "source": [
    "# V1\n",
    "model='cutin_1'\n",
    "embs = pd.read_csv(f'/neurospin/dico/jlaval/Output/SC-sylv_left/{model}/hcp_isomap_random_embeddings/full_embeddings.csv')\n",
    "#embs = pd.read_csv('/neurospin/dico/jlaval/Output/SC-sylv_left_dim/preV1_32dims/hcp_isomap_random_embeddings/full_embeddings.csv')\n",
    "n_dims = 32\n",
    "\n",
    "# V0 # would require a regularisation to make it comparable to V1\n",
    "# But what does it mean : is it better to trust CCA value of individual regressions ?\n",
    "# with 256 dimensions, the CCA is overfitting, so it should be regularized\n",
    "# If we need a large latent space to encompass the last isomap dimensions (because they could represent a small variance), then a regularization may not be enough,\n",
    "# and a regression may be better suited.\n",
    "#k=1\n",
    "#embs = pd.read_csv(f'/volatile/jl277509/Runs/02_STS_babies/Program/Output/ISOMAP_CENTRAL/22-58-04_{k}/troiani_custom_embeddings/custom_cross_val_embeddings.csv')\n",
    "#n_dims=256\n",
    "\n",
    "# reformat embeddings and isomap\n",
    "isomap = pd.read_csv('/neurospin/dico/data/deep_folding/current/datasets/hcp/hcp_isomap_labels.csv')\n",
    "embs.columns=['Subject']+embs.columns[1:].tolist()\n",
    "merged = pd.merge(isomap, embs)\n",
    "cols_embs = [f'dim{k}' for k in range(1,n_dims+1)]\n",
    "cols_iso = [f'Isomap_central_left_dim{k}' for k in range(1,7)]\n",
    "X, Y = merged[cols_embs].to_numpy(), merged[cols_iso].to_numpy()\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# fit CCA and return score\n",
    "cca = CCA(n_components=len(cols_iso), scale=False) ## need to set scale to false !!\n",
    "cca.fit(X_train, Y_train)\n",
    "# Transform the test data\n",
    "X_c, Y_c = cca.transform(X_test, Y_test)\n",
    "# Compute the canonical correlations between the canonical variables\n",
    "canonical_correlations = np.corrcoef(X_c.T, Y_c.T)[:X_c.shape[1], X_c.shape[1]:]\n",
    "# Get the average of the canonical correlations\n",
    "average_correlation = np.mean(np.diagonal(canonical_correlations))\n",
    "\n",
    "# default sklearn score\n",
    "print(f'R2 score : {cca.score(X_test, Y_test)}')\n",
    "# custom score\n",
    "print(f'Average correlation : {average_correlation}')\n",
    "print(f'Individual scores: {np.diagonal(canonical_correlations)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## keep it simple for the score as there are only 6 dims\n",
    "## NB : results consistent with linear regression on the features\n",
    "## NB : les dimensions des isomaps étant orthogonales, peut-être que la CCA n'apporte rien par rapport à des régressions linéaires sur chaque var.\n",
    "# Mais c'est plus pratique d'avoir une seule valeur\n",
    "# Devrait-on tenir compte de la taille d'effet de chaque isomap dans le score ? Ici on leur donne un poids identique.\n",
    "# Mais la CKA est trop restrictive"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
