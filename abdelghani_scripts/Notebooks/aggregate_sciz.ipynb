{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregate: /neurospin/dico/data/deep_folding/current/datasets/aggregate_schizophrenia/crops/2mm/OCCIPITAL/mask/Lskeleton_subject.csv\n",
      "Total subjects: 2406\n",
      "\n",
      "Block order in aggregate:\n",
      "  1st: cnp                       idx[0..263]  count=264\n",
      "  2nd: bsnip1                    idx[264..1566]  count=1303\n",
      "  3rd: candi                     idx[1567..1669]  count=103\n",
      "  4th: schizconnect-vip-prague   idx[1670..2405]  count=736\n",
      "\n",
      "Datasets encountered in order (collapsed):\n",
      "  cnp  ->  bsnip1  ->  candi  ->  schizconnect-vip-prague\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ==== Config ====\n",
    "ROOT = Path(\"/neurospin/dico/data/deep_folding/current/datasets\")\n",
    "ROI = \"OCCIPITAL\"\n",
    "HEMI = \"L\"\n",
    "DATASETS = [\"cnp\", \"candi\", \"bsnip1\", \"schizconnect-vip-prague\"]  # check order of appearance among these\n",
    "SUBDIR = \"crops/2mm\"\n",
    "MASKDIR = \"mask\"\n",
    "\n",
    "agg_file = ROOT / \"aggregate_schizophrenia\" / SUBDIR / ROI / MASKDIR / f\"{HEMI}skeleton_subject.csv\"\n",
    "\n",
    "HEADER_TOKENS = {\"subject\", \"subjects\", \"id\", \"ids\"}  # case-insensitive\n",
    "\n",
    "def read_ids(p: Path):\n",
    "    if not p.exists():\n",
    "        return []\n",
    "    try:\n",
    "        s = pd.read_csv(p, header=None, dtype=str, engine=\"python\")\n",
    "        col0 = s.iloc[:, 0].astype(str).str.strip()\n",
    "        vals = [x for x in col0.tolist() if x]\n",
    "    except Exception:\n",
    "        vals = [line.strip().split(\",\")[0] for line in p.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines() if line.strip()]\n",
    "    # drop header-like tokens\n",
    "    vals = [v for v in vals if v.lower() not in HEADER_TOKENS]\n",
    "    return vals\n",
    "\n",
    "# Load aggregate list\n",
    "agg_ids = read_ids(agg_file)\n",
    "\n",
    "# Build membership maps (set of IDs for each dataset)\n",
    "dataset_sets = {}\n",
    "for ds in DATASETS:\n",
    "    p = ROOT / ds / SUBDIR / ROI / MASKDIR / f\"{HEMI}skeleton_subject.csv\"\n",
    "    dataset_sets[ds] = set(read_ids(p))\n",
    "\n",
    "# Map each aggregate ID to its dataset (first matching dataset wins; else \"UNKNOWN\")\n",
    "def which_dataset(sid: str):\n",
    "    for ds in DATASETS:\n",
    "        if sid in dataset_sets[ds]:\n",
    "            return ds\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "labels = [which_dataset(sid) for sid in agg_ids]\n",
    "\n",
    "# Detect contiguous blocks by dataset label\n",
    "blocks = []\n",
    "if agg_ids:\n",
    "    cur = labels[0]; start = 0\n",
    "    for i, lab in enumerate(labels):\n",
    "        if lab != cur:\n",
    "            blocks.append((cur, start, i))  # [start, i-1]\n",
    "            cur = lab; start = i\n",
    "    blocks.append((cur, start, len(labels)))  # last block [start, end)\n",
    "\n",
    "# Print a clean “1st, 2nd, …” order with counts and indices\n",
    "suffix = [\"st\",\"nd\",\"rd\"] + [\"th\"]*10\n",
    "def ordinal(n): \n",
    "    if 10 <= (n % 100) <= 20: return f\"{n}th\"\n",
    "    return f\"{n}{suffix[min((n % 10)-1, 3)] if 1 <= (n % 10) <= 3 else 'th'}\"\n",
    "\n",
    "print(f\"Aggregate: {agg_file}\")\n",
    "print(f\"Total subjects: {len(agg_ids)}\\n\")\n",
    "\n",
    "if not blocks:\n",
    "    print(\"No subjects found.\")\n",
    "else:\n",
    "    print(\"Block order in aggregate:\")\n",
    "    order = []\n",
    "    for k, (ds, a, b) in enumerate(blocks, start=1):\n",
    "        count = b - a\n",
    "        print(f\"  {ordinal(k)}: {ds:24s}  idx[{a}..{b-1}]  count={count}\")\n",
    "        order.append(ds)\n",
    "    print(\"\\nDatasets encountered in order (collapsed):\")\n",
    "    collapsed = []\n",
    "    for ds in order:\n",
    "        if not collapsed or collapsed[-1] != ds:\n",
    "            collapsed.append(ds)\n",
    "    print(\"  \" + \"  ->  \".join(collapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote /neurospin/dico/data/deep_folding/current/datasets/aggregate_schizophrenia/crops/2mm/INSULA./mask/Lskeleton_subject.csv with 2406 subjects\n",
      "Wrote /neurospin/dico/data/deep_folding/current/datasets/aggregate_schizophrenia/crops/2mm/INSULA./mask/Lskeleton.npy with shape (2406, 27, 52, 39, 1)\n",
      " Sanity check passed for L: 2406 subjects\n",
      "Copied /neurospin/dico/data/deep_folding/current/datasets/cnp/crops/2mm/INSULA./mask/Lmask_cropped.nii.gz.minf -> /neurospin/dico/data/deep_folding/current/datasets/aggregate_schizophrenia/crops/2mm/INSULA./mask/Lmask_cropped.nii.gz.minf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "# =========================\n",
    "# Config\n",
    "# =========================\n",
    "ROOT = Path(\"/neurospin/dico/data/deep_folding/current/datasets\")\n",
    "ROI = \"INSULA.\"\n",
    "HEMIS = [\"L\"]\n",
    "DATASETS_ORDER = [\"cnp\", \"bsnip1\", \"candi\", \"schizconnect-vip-prague\"]  # order as in OCCIPITAL\n",
    "SUBDIR = \"crops/2mm\"\n",
    "MASKDIR = \"mask\"\n",
    "AGG_NAME = \"aggregate_schizophrenia\"\n",
    "\n",
    "HEADER_TOKENS = {\"subject\", \"subjects\", \"id\", \"ids\"}  # drop if seen\n",
    "\n",
    "agg_dir = ROOT / AGG_NAME / SUBDIR / ROI / MASKDIR\n",
    "agg_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def read_ids(p: Path):\n",
    "    \"\"\"Read subject IDs, drop headers, return as list\"\"\"\n",
    "    if not p.exists():\n",
    "        return []\n",
    "    try:\n",
    "        s = pd.read_csv(p, header=None, dtype=str, engine=\"python\")\n",
    "        col0 = s.iloc[:, 0].astype(str).str.strip()\n",
    "        vals = [x for x in col0.tolist() if x]\n",
    "    except Exception:\n",
    "        vals = [\n",
    "            line.strip().split(\",\")[0]\n",
    "            for line in p.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
    "            if line.strip()\n",
    "        ]\n",
    "    vals = [v for v in vals if v.lower() not in HEADER_TOKENS]\n",
    "    return vals\n",
    "\n",
    "def concat_subjects(hemi):\n",
    "    \"\"\"Return concatenated subject IDs in desired order\"\"\"\n",
    "    out = []\n",
    "    for ds in DATASETS_ORDER:\n",
    "        src = ROOT / ds / SUBDIR / ROI / MASKDIR / f\"{hemi}skeleton_subject.csv\"\n",
    "        ids = read_ids(src)\n",
    "        out.extend(ids)\n",
    "    return out\n",
    "\n",
    "# =========================\n",
    "# 1. Build subject lists\n",
    "# =========================\n",
    "all_subjects = {h: concat_subjects(h) for h in HEMIS}\n",
    "\n",
    "for h in HEMIS:\n",
    "    subj_csv = agg_dir / f\"{h}skeleton_subject.csv\"\n",
    "    with subj_csv.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for sid in all_subjects[h]:\n",
    "            f.write(f\"{sid}\\n\")\n",
    "    print(f\"Wrote {subj_csv} with {len(all_subjects[h])} subjects\")\n",
    "\n",
    "# =========================\n",
    "# 2. Build skeleton arrays (.npy)\n",
    "# =========================\n",
    "for h in HEMIS:\n",
    "    stacked_parts = []\n",
    "    subj_total = []\n",
    "    for ds in DATASETS_ORDER:\n",
    "        src_dir = ROOT / ds / SUBDIR / ROI / MASKDIR\n",
    "        ids = read_ids(src_dir / f\"{h}skeleton_subject.csv\")\n",
    "        npy_path = src_dir / f\"{h}skeleton.npy\"\n",
    "        if not npy_path.exists() or not ids:\n",
    "            print(f\"⚠️ Skipping {ds} {h}: no npy or no subjects\")\n",
    "            continue\n",
    "        arr = np.load(npy_path)  # shape should be (N_subjects, ...)\n",
    "        if arr.shape[0] != len(ids):\n",
    "            print(f\"⚠️ WARNING: mismatch in {ds} {h}: {arr.shape[0]} rows vs {len(ids)} IDs\")\n",
    "        stacked_parts.append(arr)\n",
    "        subj_total.extend(ids)\n",
    "    if stacked_parts:\n",
    "        stacked = np.concatenate(stacked_parts, axis=0)\n",
    "        out_path = agg_dir / f\"{h}skeleton.npy\"\n",
    "        np.save(out_path, stacked)\n",
    "        print(f\"Wrote {out_path} with shape {stacked.shape}\")\n",
    "        # sanity check\n",
    "        agg_ids = read_ids(agg_dir / f\"{h}skeleton_subject.csv\")\n",
    "        if len(agg_ids) != stacked.shape[0]:\n",
    "            print(f\" ERROR: mismatch for {h}: {len(agg_ids)} IDs vs {stacked.shape[0]} rows\")\n",
    "        else:\n",
    "            print(f\" Sanity check passed for {h}: {len(agg_ids)} subjects\")\n",
    "    else:\n",
    "        print(f\" No arrays found for {h}, nothing written\")\n",
    "\n",
    "# =========================\n",
    "# 3. Copy mask .minf\n",
    "# =========================\n",
    "for h in HEMIS:\n",
    "    src_minf = ROOT / DATASETS_ORDER[0] / SUBDIR / ROI / MASKDIR / f\"{h}mask_cropped.nii.gz.minf\"\n",
    "    dst_minf = agg_dir / f\"{h}mask_cropped.nii.gz.minf\"\n",
    "    if src_minf.exists():\n",
    "        shutil.copy(src_minf, dst_minf)\n",
    "        print(f\"Copied {src_minf} -> {dst_minf}\")\n",
    "    else:\n",
    "        print(f\"⚠️ Missing mask .minf for {h} in {DATASETS_ORDER[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[L] copied mask_cropped.nii.gz from /neurospin/dico/data/deep_folding/current/datasets/cnp/crops/2mm/INSULA./mask/Lmask_cropped.nii.gz\n",
      "[L] copied mask_cropped.nii.gz.minf from /neurospin/dico/data/deep_folding/current/datasets/cnp/crops/2mm/INSULA./mask/Lmask_cropped.nii.gz.minf\n",
      "[L] copied mask_skeleton.nii.gz from /neurospin/dico/data/deep_folding/current/datasets/cnp/crops/2mm/INSULA./mask/Lmask_skeleton.nii.gz\n",
      "[L] copied mask_skeleton.nii.gz.minf from /neurospin/dico/data/deep_folding/current/datasets/cnp/crops/2mm/INSULA./mask/Lmask_skeleton.nii.gz.minf\n",
      "[L] wrote /neurospin/dico/data/deep_folding/current/datasets/aggregate_schizophrenia/crops/2mm/INSULA./mask/Lskeleton_subject.csv with 2406 subjects\n",
      "[L] no Lcrops directory found in any dataset; skipped\n",
      "\n",
      "Done. If something is still missing, it means no dataset provided that artifact for the given ROI/hemi.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ========= config =========\n",
    "ROOT = Path(\"/neurospin/dico/data/deep_folding/current/datasets\")\n",
    "AGG_NAME = \"aggregate_schizophrenia\"\n",
    "SUBDIR = \"crops/2mm\"\n",
    "ROI = \"INSULA.\"                   # <- you used INSULA. (with the dot)\n",
    "MASKDIR = \"mask\"\n",
    "DATASETS_ORDER = [\"cnp\", \"bsnip1\", \"candi\", \"schizconnect-vip-prague\"]\n",
    "HEMIS = [\"L\"]                # will skip missing ones automatically\n",
    "USE_SYMLINKS = True               # set False to copy files instead of symlink\n",
    "\n",
    "agg_mask_dir = ROOT / AGG_NAME / SUBDIR / ROI / MASKDIR\n",
    "agg_mask_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def first_existing(path_list):\n",
    "    for p in path_list:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "def copy_or_link(src: Path, dst: Path):\n",
    "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if dst.exists():\n",
    "        return\n",
    "    if USE_SYMLINKS:\n",
    "        try:\n",
    "            os.symlink(src, dst)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "    else:\n",
    "        shutil.copy2(src, dst)\n",
    "\n",
    "# 1) Copy common mask files per hemisphere (take the first dataset that has them)\n",
    "for h in HEMIS:\n",
    "    candidates = [ROOT / ds / SUBDIR / ROI / MASKDIR for ds in DATASETS_ORDER]\n",
    "\n",
    "    mask_nii_src  = first_existing([c / f\"{h}mask_cropped.nii.gz\"      for c in candidates])\n",
    "    mask_inf_src  = first_existing([c / f\"{h}mask_cropped.nii.gz.minf\" for c in candidates])\n",
    "    skel_nii_src  = first_existing([c / f\"{h}mask_skeleton.nii.gz\"      for c in candidates])\n",
    "    skel_inf_src  = first_existing([c / f\"{h}mask_skeleton.nii.gz.minf\" for c in candidates])\n",
    "\n",
    "    if mask_nii_src:\n",
    "        copy_or_link(mask_nii_src, agg_mask_dir / f\"{h}mask_cropped.nii.gz\")\n",
    "        print(f\"[{h}] copied mask_cropped.nii.gz from {mask_nii_src}\")\n",
    "    else:\n",
    "        print(f\"[{h}] no mask_cropped.nii.gz found in any dataset\")\n",
    "\n",
    "    if mask_inf_src:\n",
    "        copy_or_link(mask_inf_src, agg_mask_dir / f\"{h}mask_cropped.nii.gz.minf\")\n",
    "        print(f\"[{h}] copied mask_cropped.nii.gz.minf from {mask_inf_src}\")\n",
    "    else:\n",
    "        print(f\"[{h}] no mask_cropped.nii.gz.minf found in any dataset\")\n",
    "\n",
    "    if skel_nii_src:\n",
    "        copy_or_link(skel_nii_src, agg_mask_dir / f\"{h}mask_skeleton.nii.gz\")\n",
    "        print(f\"[{h}] copied mask_skeleton.nii.gz from {skel_nii_src}\")\n",
    "    else:\n",
    "        print(f\"[{h}] no mask_skeleton.nii.gz found in any dataset\")\n",
    "\n",
    "    if skel_inf_src:\n",
    "        copy_or_link(skel_inf_src, agg_mask_dir / f\"{h}mask_skeleton.nii.gz.minf\")\n",
    "        print(f\"[{h}] copied mask_skeleton.nii.gz.minf from {skel_inf_src}\")\n",
    "    else:\n",
    "        print(f\"[{h}] no mask_skeleton.nii.gz.minf found in any dataset\")\n",
    "\n",
    "# 2) Merge Lcrops/Rcrops directories\n",
    "for h in HEMIS:\n",
    "    agg_crops_dir = agg_mask_dir / f\"{h}crops\"\n",
    "    added = 0\n",
    "    for ds in DATASETS_ORDER:\n",
    "        src_crops = ROOT / ds / SUBDIR / ROI / MASKDIR / f\"{h}crops\"\n",
    "        if not src_crops.exists():\n",
    "            continue\n",
    "        for item in sorted(src_crops.iterdir()):\n",
    "            dst = agg_crops_dir / item.name\n",
    "            if dst.exists():\n",
    "                continue\n",
    "            if item.is_dir():\n",
    "                if USE_SYMLINKS:\n",
    "                    os.symlink(item, dst)\n",
    "                else:\n",
    "                    shutil.copytree(item, dst)\n",
    "            else:\n",
    "                copy_or_link(item, dst)\n",
    "            added += 1\n",
    "    if added > 0:\n",
    "        print(f\"[{h}] merged {added} entries into {agg_crops_dir}\")\n",
    "    else:\n",
    "        print(f\"[{h}] no {h}crops directory found in any dataset; skipped\")\n",
    "\n",
    "# 3) Reformat skeleton_subject.csv files with \"Subject\" header\n",
    "for h in HEMIS:\n",
    "    subj_csv = agg_mask_dir / f\"{h}skeleton_subject.csv\"\n",
    "    if subj_csv.exists():\n",
    "        ids = [line.strip() for line in subj_csv.read_text().splitlines() if line.strip() and line.lower() != \"subject\"]\n",
    "        pd.DataFrame(ids, columns=[\"Subject\"]).to_csv(subj_csv, index=False)\n",
    "        print(f\"[{h}] rewrote {subj_csv} with header 'Subject' and {len(ids)} rows\")\n",
    "\n",
    "print(\"\\nDone. If something is still missing, it means no dataset provided that artifact for the given ROI/hemi.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "contrastive_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
